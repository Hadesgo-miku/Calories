{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from regex import F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDS  = [42, 2025, 7]\n",
    "FOLDS  = 5\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "ARCH   = [128, 128, 128, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [c for c in train_data.columns if c not in ['id', 'Calories']]\n",
    "TARGET   = 'Calories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集形状: (600000, 7)\n",
      "验证集形状: (150000, 7)\n"
     ]
    }
   ],
   "source": [
    "X=train_data[FEATURES]\n",
    "y=train_data[TARGET]\n",
    "\n",
    "\n",
    "RANDOM_STATE=42\n",
    "# 使用train_test_split函数进行数据集分割\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,  # 验证集占比20%\n",
    "    random_state=RANDOM_STATE  # 设置随机种子\n",
    ")\n",
    "# 打印数据集形状以验证分割结果\n",
    "print(f\"训练集形状: {X_tr.shape}\")\n",
    "print(f\"验证集形状: {X_va.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr=X_tr.values.astype(np.float32)\n",
    "X_va=X_va.values.astype(np.float32)\n",
    "y_tr=y_tr.values.astype(np.float32)\n",
    "y_va=y_va.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(y_tr))\n",
    "valid_ds = TensorDataset(torch.from_numpy(X_va), torch.from_numpy(y_va))\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    残差块模块\n",
    "    参数:\n",
    "        in_f: 输入特征维度\n",
    "        u1: 第一个全连接层的输出维度\n",
    "        u2: 第二个全连接层的输出维度\n",
    "    \"\"\"\n",
    "    def __init__(self, in_f, u1, u2):\n",
    "        super().__init__()\n",
    "        # 第一个全连接层\n",
    "        self.fc1 = nn.Linear(in_f, u1, bias=False)\n",
    "        # 第一个批归一化层\n",
    "        self.bn1 = nn.BatchNorm1d(u1)\n",
    "        # 第一个激活函数层(SiLU)\n",
    "        self.act1 = nn.SiLU(inplace=True)\n",
    "        \n",
    "        # 第二个全连接层\n",
    "        self.fc2 = nn.Linear(u1, u2, bias=False)\n",
    "        # 第二个批归一化层\n",
    "        self.bn2 = nn.BatchNorm1d(u2)\n",
    "        \n",
    "        # 如果输入维度不等于输出维度,需要投影层进行维度匹配\n",
    "        if in_f != u2:\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.Linear(in_f, u2, bias=False),\n",
    "                nn.BatchNorm1d(u2)\n",
    "            )\n",
    "        else:\n",
    "            self.proj = None\n",
    "            \n",
    "        # 输出激活函数层\n",
    "        self.act_out = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 主路径处理\n",
    "        y = self.fc1(x)\n",
    "        y = self.bn1(y)\n",
    "        y = self.act1(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.bn2(y)\n",
    "        \n",
    "        # 残差连接处理\n",
    "        shortcut = self.proj(x) if self.proj is not None else x\n",
    "        \n",
    "        # 残差连接并激活\n",
    "        return self.act_out(y + shortcut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, preds: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # 保证非负\n",
    "        preds = preds.clamp(min=0.0)\n",
    "        # log1p 变换\n",
    "        log_preds   = torch.log1p(preds + self.eps)\n",
    "        log_targets = torch.log1p(targets + self.eps)\n",
    "        # 均方误差后开根号\n",
    "        return torch.sqrt(F.mse_loss(log_preds, log_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    早停机制类\n",
    "    用于在模型训练过程中监控验证集损失,当损失不再下降时提前停止训练\n",
    "    参数:\n",
    "        patience: 容忍验证集损失不下降的轮数,默认为10\n",
    "        delta: 判断损失改善的最小阈值,默认为0.0\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, delta=0.0):\n",
    "        # 初始化早停参数\n",
    "        self.patience = patience  # 容忍轮数\n",
    "        self.delta = delta  # 最小改善阈值\n",
    "        self.best_loss = float('inf')  # 记录最佳损失值,初始化为无穷大\n",
    "        self.counter = 0  # 计数器,记录损失未改善的轮数\n",
    "        self.best_state = None  # 存储最佳模型状态\n",
    "\n",
    "    def step(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        每轮训练后调用,判断是否需要早停\n",
    "        参数:\n",
    "            val_loss: 当前验证集损失\n",
    "            model: 当前模型\n",
    "        返回:\n",
    "            bool: 是否需要早停\n",
    "        \"\"\"\n",
    "        # 判断当前损失是否优于历史最佳损失(考虑delta阈值)\n",
    "        if val_loss + self.delta < self.best_loss:\n",
    "            # 更新最佳损失和模型状态\n",
    "            self.best_loss = val_loss\n",
    "            # 将模型参数转移到CPU并保存\n",
    "            self.best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "            self.counter = 0  # 重置计数器\n",
    "        else:\n",
    "            # 损失未改善,计数器加1\n",
    "            self.counter += 1\n",
    "        # 返回是否达到早停条件\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "    def restore(self, model):\n",
    "        \"\"\"\n",
    "        恢复最佳模型状态\n",
    "        参数:\n",
    "            model: 需要恢复的模型\n",
    "        \"\"\"\n",
    "        # 加载保存的最佳模型参数\n",
    "        model.load_state_dict(self.best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResMLPRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    残差多层感知机回归器\n",
    "    实现了带有残差连接的深度神经网络结构\n",
    "    参数:\n",
    "        input_dim: 输入特征的维度\n",
    "        units: 隐藏层单元数的列表,每两个数字代表一个残差块的输入输出维度\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, units):\n",
    "        super().__init__()\n",
    "        # 输入层批归一化,用于规范化输入特征分布\n",
    "        self.input_bn = nn.BatchNorm1d(input_dim)\n",
    "        \n",
    "        # 构建残差块序列\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        # 每两个数字构建一个残差块\n",
    "        for i in range(0, len(units), 2):\n",
    "            # 获取当前残差块的输入输出维度\n",
    "            u1 = units[i]  # 第一个维度\n",
    "            u2 = units[i+1] if i+1<len(units) else units[i]  # 第二个维度,如果不存在则使用第一个维度\n",
    "            # 添加残差块\n",
    "            layers.append(ResidualBlock(in_dim, u1, u2))\n",
    "            in_dim = u2  # 更新下一层的输入维度\n",
    "        \n",
    "        # 将所有残差块组合成序列\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        # 输出层,将特征映射到单个预测值\n",
    "        self.head = nn.Linear(in_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播过程\n",
    "        参数:\n",
    "            x: 输入特征张量\n",
    "        返回:\n",
    "            预测值张量\n",
    "        \"\"\"\n",
    "        x = self.input_bn(x)     # 输入特征批归一化\n",
    "        x = self.blocks(x)       # 通过残差块序列\n",
    "        return self.head(x).squeeze(-1)  # 输出层预测并压缩维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化残差多层感知机模型\n",
    "# input_dim设置为训练数据特征维度,units使用预定义的网络架构ARCH\n",
    "model = ResMLPRegressor(input_dim=X_tr.shape[1], units=ARCH)\n",
    "\n",
    "# 定义均方根对数误差损失函数\n",
    "criterion = RMSLELoss()\n",
    "\n",
    "# 使用Adam优化器,学习率设为0.001\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 学习率调度器\n",
    "# 当验证损失停止下降时,将学习率降低为原来的一半\n",
    "# patience=3表示等待3个epoch后仍未改善则降低学习率\n",
    "# min_lr=1e-6设置最小学习率下限\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# 早停机制\n",
    "# patience=10表示如果验证损失在10个epoch内没有改善则停止训练\n",
    "stopper = EarlyStopping(patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "model = ResMLPRegressor(input_dim=X_tr.shape[1], units=ARCH).to(device)\n",
    "criterion = RMSLELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        \n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "\n",
    "    # 验证\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in valid_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            vp = model(xb)\n",
    "            val_losses.append(criterion(vp, yb).item())\n",
    "\n",
    "    avg_val_loss = float(sum(val_losses) / len(val_losses))\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(\n",
    "        f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "        f\"Val Loss:   {avg_val_loss:.4f} | \"\n",
    "        f\"LR: {current_lr:.1e} | \"\n",
    "        f\"Time: {elapsed:.1f}s\"\n",
    "    )\n",
    "\n",
    "    if stopper.step(avg_val_loss, model):\n",
    "        stopper.restore(model)\n",
    "        print(f\"⏹ Early stopping at epoch {epoch}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
